import React, { useState, useEffect, useRef } from "react";
import {
┬а Mic,
┬а MicOff,
┬а Bot,
┬а Loader2,
┬а Globe,
┬а AlertTriangle,
} from "lucide-react";

// --- CONFIGURATION ---
const USE_AI = true;
// NOTE: Use a low-latency model for voice assistants like gemini-flash
const AI_MODEL = "gemini-pro"; // Keep 'gemini-pro' as a stable default if flash endpoint is complex.
// The base endpoint for the Google GenAI API (using generateText for simplicity here, but a custom streaming solution is ideal)
const API_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models";

/**
┬а* ЁЯТб Simulates a streaming AI call for better perceived performance.
┬а*/
async function callAIModel(
┬а text: string,
┬а lang = "en-IN",
┬а onChunk: (chunk: string) => void,
┬а onEnd: (fullText: string | null) => void
) {
┬а try {
┬а ┬а if (!USE_AI) {
┬а ┬а ┬а ┬а onEnd(null);
┬а ┬а ┬а ┬а return null;
┬а ┬а }

┬а ┬а const apiKey = import.meta.env.VITE_GEMINI_API_KEY;
┬а ┬а if (!apiKey) {
┬а ┬а ┬а throw new Error("API Key not found in environment variables.");
┬а ┬а }

┬а ┬а // Map user query to the language prompt for the model
┬а ┬а const promptMap: any = {
┬а ┬а ┬а "hi-IN": `Respond concisely in Hindi: ${text}`,
┬а ┬а ┬а "mr-IN": `Respond concisely in Marathi: ${text}`,
┬а ┬а ┬а "kn-IN": `Respond concisely in Kannada: ${text}`,
┬а ┬а ┬а "en-IN": text,
┬а ┬а };

┬а ┬а const res = await fetch(
┬а ┬а ┬а `${API_BASE_URL}/${AI_MODEL}:generateContent?key=${apiKey}`,
┬а ┬а ┬а {
┬а ┬а ┬а ┬а method: "POST",
┬а ┬а ┬а ┬а body: JSON.stringify({
┬а ┬а ┬а ┬а ┬а contents: [{ role: "user", parts: [{ text: promptMap[lang] || text }] }],
┬а ┬а ┬а ┬а ┬а config: {
┬а ┬а ┬а ┬а ┬а ┬а // Set a low max output to keep response time fast
┬а ┬а ┬а ┬а ┬а ┬а maxOutputTokens: 150, 
┬а ┬а ┬а ┬а ┬а }
┬а ┬а ┬а ┬а }),
┬а ┬а ┬а }
┬а ┬а );

┬а ┬а if (!res.ok) {
┬а ┬а ┬а ┬а const errorData = await res.json();
┬а ┬а ┬а ┬а throw new Error(`AI API Error: ${res.status} - ${errorData.error.message || 'Unknown error'}`);
┬а ┬а }

┬а ┬а const data = await res.json();
┬а ┬а const fullText = data.candidates?.[0]?.content?.parts?.[0]?.text || null;

┬а ┬а if (fullText) {
┬а ┬а ┬а ┬а // Simulate streaming (For true streaming, this logic would process the response stream)
┬а ┬а ┬а ┬а const words = fullText.split(/\s+/);
┬а ┬а ┬а ┬а let currentText = "";
┬а ┬а ┬а ┬а for (const word of words) {
┬а ┬а ┬а ┬а ┬а ┬а currentText += word + " ";
┬а ┬а ┬а ┬а ┬а ┬а onChunk(currentText.trim());
┬а ┬а ┬а ┬а ┬а ┬а // Small pause to simulate real-time streaming effect
┬а ┬а ┬а ┬а ┬а ┬а await new Promise(resolve => setTimeout(resolve, 30)); 
┬а ┬а ┬а ┬а }
┬а ┬а }
┬а ┬а 
┬а ┬а onEnd(fullText);
┬а ┬а return fullText;

┬а } catch (err) {
┬а ┬а console.error("Error in callAIModel:", err);
┬а ┬а onEnd(null); // End the process on failure
┬а ┬а throw err; // Re-throw to be caught by the caller
┬а }
}

// ----------------------------------------------------
// SMART LOCAL RESPONSES (WEATHER, MARKET, DISEASE, ETC)
// ----------------------------------------------------
function getLocalSmartResponse(text: string, lang: string) {
┬а const lower = text.toLowerCase();

┬а // WEATHER - Highly optimized for speed
┬а if (
┬а ┬а /\bweather\b|\brain\b|\bforecast\b/.test(lower) ||
┬а ┬а /(рдмрд╛рд░рд┐рд╢|рдореМрд╕рдо|рд╣рд╡рд╛рдорд╛рди|рдкрд╛рдКрд╕|р▓ор▓│р│Ж|р▓╣р▓╡р▓╛р▓ор▓╛р▓и)/.test(text)
┬а ) {
┬а ┬а // Data for Vasai-Virar/Palghar area (Nov 25, 2025)
┬а ┬а return {
┬а ┬а ┬а "hi-IN": "рдЖрдЬ рдореБрдореНрдмрдИ рдореЗрдВ: рдЖрд╕рдорд╛рди рдореЗрдВ рдзреВрдк рд░рд╣реЗрдЧреА, рдЕрдзрд┐рдХрддрдо рддрд╛рдкрдорд╛рди 31 рдбрд┐рдЧреНрд░реА рд╕реЗрд▓реНрд╕рд┐рдпрд╕ рддрдерд╛ рдиреНрдпреВрдирддрдо рддрд╛рдкрдорд╛рди 23 рдбрд┐рдЧреНрд░реА рд╕реЗрд▓реНрд╕рд┐рдпрд╕ рдХреЗ рдЖрд╕рдкрд╛рд╕ рд░рд╣реЗрдЧрд╛ред",
┬а ┬а ┬а "mr-IN": "рдЖрдЬ рдореБрдВрдмрдИрдд: рдЖрдХрд╛рд╢ рд╕реВрд░реНрдпрдкреНрд░рдХрд╛рд╢рд┐рдд рдЕрд╕реЗрд▓, рдХрдорд╛рд▓ рддрд╛рдкрдорд╛рди рд╕реБрдорд╛рд░реЗ рейрез рдЕрдВрд╢ рд╕реЗрд▓реНрд╕рд┐рдЕрд╕ рдЖрдгрд┐ рдХрд┐рдорд╛рди рддрд╛рдкрдорд╛рди рд╕реБрдорд╛рд░реЗ реирей рдЕрдВрд╢ рд╕реЗрд▓реНрд╕рд┐рдЕрд╕ рд░рд╛рд╣реАрд▓.",
┬а ┬а ┬а "kn-IN": "р▓Зр▓Вр▓жр│Б р▓ор│Бр▓Вр▓мр│Ир▓ир▓▓р│Нр▓▓р▓┐: р▓мр▓┐р▓╕р▓┐р▓▓р▓┐р▓и р▓Жр▓Хр▓╛р▓╢, р▓Чр▓░р▓┐р▓╖р│Нр▓а р▓др▓╛р▓кр▓ор▓╛р▓и р▓╕р│Бр▓ор▓╛р▓░р│Б 31 ┬░C р▓ор▓др│Нр▓др│Б р▓Хр▓ир▓┐р▓╖р│Нр▓а р▓др▓╛р▓кр▓ор▓╛р▓и р▓╕р│Бр▓ор▓╛р▓░р│Б 23 ┬░C.",
┬а ┬а ┬а "en-IN": "Today in Mumbai: Sunny skies with a high around 31 ┬░C and low near 23 ┬░C.",
┬а ┬а }[lang];
┬а }

┬а // MARKET RATES
┬а if (
┬а ┬а /\bprice\b|\bmarket\b|\brate\b/.test(lower) ||
┬а ┬а /(рднрд╛рд╡|рдХрд┐рдВрдордд|рджрд░|рдорд╛рд░реНрдХреЗрдЯ|р▓Хр▓┐р▓ор│Нр▓ор▓др│Нр▓др│Б)/.test(text)
┬а ) {
┬а ┬а // Data based on Mumbai APMC (Navi Mumbai) rates (Nov 25, 2025)
┬а ┬а if (/tomato|рдЯрдорд╛рдЯрд░|рдЯреЛрдореЕрдЯреЛ|р▓Яр│Кр▓ор│Жр▓Яр│К/.test(text)) {
┬а ┬а ┬а return {
┬а ┬а ┬а ┬а "hi-IN": "рдЖрдЬ рдЯрдорд╛рдЯрд░ рдХрд╛ рд░реЗрдЯ (рдореБрдВрдмрдИ APMC) тВ╣48/kg рд╣реИред",
┬а ┬а ┬а ┬а "mr-IN": "рдЖрдЬ рдЯреЛрдореЕрдЯреЛрдЪреА рдХрд┐рдВрдордд (рдореБрдВрдмрдИ APMC) тВ╣48/kg рдЖрд╣реЗ.",
┬а ┬а ┬а ┬а "kn-IN": "р▓Зр▓Вр▓жр│Б р▓Яр│Кр▓ор│Зр▓Яр│К р▓мр│Жр▓▓р│Ж (р▓ор│Бр▓Вр▓мр│И APMC) тВ╣р│кр│о р▓Хр▓┐р▓▓р│Лр▓Чр│Ж.",
┬а ┬а ┬а ┬а "en-IN": "Today's tomato price (Mumbai APMC) is тВ╣48 per kg.",
┬а ┬а ┬а }[lang];
┬а ┬а }
┬а ┬а if (/onion|рдкреНрдпрд╛рдЬ|рдХрд╛рдВрджрд╛|р▓Ир▓░р│Бр▓│р│Нр▓│р▓┐/.test(text)) {
┬а ┬а ┬а return {
┬а ┬а ┬а ┬а "hi-IN": "рдЖрдЬ рдкреНрдпрд╛рдЬ рдХрд╛ рд░реЗрдЯ (рдореБрдВрдмрдИ APMC) тВ╣28/kg рд╣реИред",
┬а ┬а ┬а ┬а "mr-IN": "рдЖрдЬ рдХрд╛рдВрджреНрдпрд╛рдЪреА рдХрд┐рдВрдордд (рдореБрдВрдмрдИ APMC) тВ╣28/kg рдЖрд╣реЗ.",
┬а ┬а ┬а ┬а "kn-IN": "р▓Зр▓Вр▓жр│Б р▓Ир▓░р│Бр▓│р│Нр▓│р▓┐ р▓мр│Жр▓▓р│Ж (р▓ор│Бр▓Вр▓мр│И APMC) тВ╣р│ир│о р▓Хр▓┐р▓▓р│Лр▓Чр│Ж.",
┬а ┬а ┬а ┬а "en-IN": "Today's onion price (Mumbai APMC) is тВ╣28 per kg.",
┬а ┬а ┬а }[lang];
┬а ┬а }
┬а ┬а return {
┬а ┬а ┬а "hi-IN": "рдХреМрди рд╕рд╛ рд░реЗрдЯ рдЪрд╛рд╣рд┐рдП? рдЙрджрд╛рд╣рд░рдг: рдЯрдорд╛рдЯрд░ рдХрд╛ рд░реЗрдЯ рдпрд╛ рдкреНрдпрд╛рдЬ рдХрд╛ рд░реЗрдЯ",
┬а ┬а ┬а "mr-IN": "рдХреБрдард▓рд╛ рднрд╛рд╡ рдкрд╛рд╣рд┐рдЬреЗ? рдЙрджрд╛рд╣рд░рдг: рдЯреЛрдореЕрдЯреЛрдЪрд╛ рднрд╛рд╡ рдХрд┐рдВрд╡рд╛ рдХрд╛рдВрджреНрдпрд╛рдЪрд╛ рднрд╛рд╡",
┬а ┬а ┬а "kn-IN": "р▓пр▓╛р▓╡ р▓мр│Жр▓▓р│Ж р▓мр│Зр▓Хр│Б? р▓Йр▓жр▓╛р▓╣р▓░р▓гр│Жр▓Чр│Ж: р▓Яр│Кр▓ор│Зр▓Яр│К р▓мр│Жр▓▓р│Ж",
┬а ┬а ┬а "en-IN": "Which price do you want? E.g., tomato or onion price.",
┬а ┬а }[lang];
┬а }

┬а // DISEASE / PEST
┬а if (
┬а ┬а /disease|yellow|spots|infection|pest|рдереНрд░рд┐рдкреНрд╕|рд░реЛрдЧ|рдкрд╛рди|рдкрд┐рд╡рд│реЗ|рдбрд╛рдЧ|р▓░р│Лр▓Ч|р▓╣р▓│р▓жр▓┐/.test(text)
┬а ) {
┬а ┬а return {
┬а ┬а ┬а "hi-IN": "рдпрд╣ рдкрддреНрддреЛрдВ рдкрд░ рдереНрд░рд┐рдкреНрд╕ (Thrips) рдЬреИрд╕рд╛ рджрд┐рдЦ рд░рд╣рд╛ рд╣реИ, рдЬреЛ рдорд╣рд╛рд░рд╛рд╖реНрдЯреНрд░ рдХреА рдлрд╕рд▓реЛрдВ рдореЗрдВ рдЖрдо рд╣реИред рд░реЛрдХрдерд╛рдо рдХреЗ рд▓рд┐рдП рдиреАрдо рддреЗрд▓ рдФрд░ рд▓рд╣рд╕реБрди рдХрд╛ рдШреЛрд▓ рдЫрд┐рдбрд╝рдХрд╛рд╡ рдХрд░реЗрдВред",
┬а ┬а ┬а "mr-IN": "рд╣реЗ рдереНрд░рд┐рдкреНрд╕ (Thrips) рд░реЛрдЧрд╛рдЪреЗ рд▓рдХреНрд╖рдг рдЖрд╣реЗ, рдЬреЛ рдорд╣рд╛рд░рд╛рд╖реНрдЯреНрд░рд╛рддреАрд▓ рдкрд┐рдХрд╛рдВрд╕рд╛рдареА рд╕рд╛рдорд╛рдиреНрдп рдЖрд╣реЗ. рдирд┐рдпрдВрддреНрд░рдгрд╛рд╕рд╛рдареА рдирд┐рдо рддреЗрд▓ рдЖрдгрд┐ рд▓рд╕рдгрд╛рдЪреЗ рджреНрд░рд╛рд╡рдг рдлрд╡рд╛рд░рдгреА рдХрд░рд╛.",
┬а ┬а ┬а "kn-IN": "р▓Зр▓жр│Б р▓ер│Нр▓░р▓┐р▓кр│Нр▓╕р│Н (Thrips) р▓░р│Лр▓Чр▓▓р▓Хр│Нр▓╖р▓гр▓Чр▓│р▓Вр▓др│Ж р▓Хр▓╛р▓гр│Бр▓др│Нр▓др▓жр│Ж, р▓Зр▓жр│Б р▓ор▓╣р▓╛р▓░р▓╛р▓╖р│Нр▓Яр│Нр▓░р▓ж р▓мр│Жр▓│р│Жр▓Чр▓│р▓▓р│Нр▓▓р▓┐ р▓╕р▓╛р▓ор▓╛р▓ир│Нр▓пр▓╡р▓╛р▓Чр▓┐р▓жр│Ж. р▓ир▓┐р▓пр▓Вр▓др│Нр▓░р▓гр▓Хр│Нр▓Хр▓╛р▓Чр▓┐ р▓мр│Зр▓╡р▓┐р▓и р▓Ор▓гр│Нр▓гр│Ж р▓ор▓др│Нр▓др│Б р▓мр│Жр▓│р│Нр▓│р│Бр▓│р│Нр▓│р▓┐ р▓жр│Нр▓░р▓╛р▓╡р▓гр▓╡р▓ир│Нр▓ир│Б р▓╕р▓┐р▓Вр▓кр▓бр▓┐р▓╕р▓┐.",
┬а ┬а ┬а "en-IN": "These look like Thrips symptoms, common in Maharashtra crops. Spray Neem oil and garlic solution for control.",
┬а ┬а }[lang];
┬а }

┬а return null;
}

// -----------------------------------
// MAIN COMPONENT
// -----------------------------------
export const VoiceAssistant = () => {
┬а const [isListening, setIsListening] = useState(false);
┬а const [transcript, setTranscript] = useState("");
┬а const [response, setResponse] = useState("");
┬а const [language, setLanguage] = useState("en-IN");
┬а const [loading, setLoading] = useState(false);
┬а const [error, setError] = useState<string | null>(null);

┬а const recognitionRef = useRef<any>(null);
┬а const finalTranscriptRef = useRef<string>(""); // NEW: Store the final transcript outside of state

┬а // SPEECH RECOGNITION
┬а useEffect(() => {
┬а ┬а if (!isListening) return;

┬а ┬а const SpeechRecognition =
┬а ┬а ┬а (window as any).SpeechRecognition ||
┬а ┬а ┬а (window as any).webkitSpeechRecognition;

┬а ┬а if (!SpeechRecognition) {
┬а ┬а ┬а setError("Speech Recognition is not supported by this browser.");
┬а ┬а ┬а setIsListening(false);
┬а ┬а ┬а return;
┬а ┬а }

┬а ┬а const recognition = new SpeechRecognition();
┬а ┬а recognitionRef.current = recognition;
┬а ┬а setError(null);
┬а ┬а setResponse("");
┬а ┬а setTranscript("");
┬а ┬а finalTranscriptRef.current = ""; // Reset final transcript

┬а ┬а recognition.continuous = false;
┬а ┬а recognition.interimResults = true;
┬а ┬а recognition.lang = language;

┬а ┬а recognition.onresult = (e: any) => {
┬а ┬а ┬а let liveTranscript = "";
┬а ┬а ┬а let finalTranscript = "";
┬а ┬а ┬а 
┬а ┬а ┬а for (let i = 0; i < e.results.length; i++) {
┬а ┬а ┬а ┬а const t = e.results[i][0].transcript;
┬а ┬а ┬а ┬а liveTranscript += t;
┬а ┬а ┬а ┬а 
┬а ┬а ┬а ┬а // Capture the most confident, final result
┬а ┬а ┬а ┬а if (e.results[i].isFinal) {
┬а ┬а ┬а ┬а ┬а ┬а finalTranscript += t;
┬а ┬а ┬а ┬а }
┬а ┬а ┬а }
┬а ┬а ┬а 
┬а ┬а ┬а setTranscript(liveTranscript);
┬а ┬а ┬а // Store the final transcript immediately
┬а ┬а ┬а finalTranscriptRef.current = finalTranscript || liveTranscript;
┬а ┬а };

┬а ┬а recognition.onerror = (e: any) => {
┬а ┬а ┬а ┬а setIsListening(false);
┬а ┬а ┬а ┬а setError(`Recognition Error: ${e.error}`);
┬а ┬а }

┬а ┬а recognition.onend = () => {
┬а ┬а ┬а setIsListening(false);
┬а ┬а ┬а const definitiveTranscript = finalTranscriptRef.current;
┬а ┬а ┬а 
┬а ┬а ┬а // FIX: Use the definitive transcript reference, not the state
┬а ┬а ┬а if (definitiveTranscript.trim()) {
┬а ┬а ┬а ┬а handleVoiceCommand(definitiveTranscript);
┬а ┬а ┬а }
┬а ┬а };

┬а ┬а recognition.start();
┬а ┬а return () => {
┬а ┬а ┬а ┬а // Cleanup: Abort if the component unmounts or listening state changes
┬а ┬а ┬а ┬а if (recognitionRef.current) {
┬а ┬а ┬а ┬а ┬а ┬а recognitionRef.current.abort();
┬а ┬а ┬а ┬а }
┬а ┬а };
┬а }, [isListening, language]); 

┬а // SPEAK (Text-to-Speech)
┬а const speak = (text: string, lang: string) => {
┬а ┬а const utter = new SpeechSynthesisUtterance(text);
┬а ┬а utter.lang = lang;
┬а ┬а // Always cancel the previous speech before starting a new one
┬а ┬а speechSynthesis.cancel();
┬а ┬а speechSynthesis.speak(utter);
┬а };

┬а // PROCESS COMMAND
┬а const handleVoiceCommand = async (text: string) => {
┬а ┬а setLoading(true);
┬а ┬а setError(null);
┬а ┬а setResponse(""); // Clear old response

┬а ┬а const local = getLocalSmartResponse(text, language);
┬а ┬а 
┬а ┬а if (local) {
┬а ┬а ┬а // 1. Instant Local Response (Fastest)
┬а ┬а ┬а setResponse(local);
┬а ┬а ┬а speak(local, language);
┬а ┬а ┬а setLoading(false);
┬а ┬а ┬а return;
┬а ┬а }

┬а ┬а // 2. Fallback to AI Model (Slower, but uses streaming simulation for better UX)
┬а ┬а try {
┬а ┬а ┬а ┬а let finalResponseText: string | null = null;

┬а ┬а ┬а ┬а await callAIModel(
┬а ┬а ┬а ┬а ┬а ┬а text, 
┬а ┬а ┬а ┬а ┬а ┬а language,
┬а ┬а ┬а ┬а ┬а ┬а // onChunk is called repeatedly as text arrives
┬а ┬а ┬а ┬а ┬а ┬а (chunk) => setResponse(chunk),
┬а ┬а ┬а ┬а ┬а ┬а // onEnd is called when the AI response is complete or an error occurs
┬а ┬а ┬а ┬а ┬а ┬а (fullText) => {
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а finalResponseText = fullText;
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а setLoading(false);
┬а ┬а ┬а ┬а ┬а ┬а }
┬а ┬а ┬а ┬а );

┬а ┬а ┬а ┬а // After streaming is complete and loading is false, speak the final response.
┬а ┬а ┬а ┬а if (finalResponseText) {
┬а ┬а ┬а ┬а ┬а ┬а ┬аspeak(finalResponseText, language);
┬а ┬а ┬а ┬а }

┬а ┬а } catch (err: any) {
┬а ┬а ┬а ┬а setLoading(false);
┬а ┬а ┬а ┬а const fallback = {
┬а ┬а ┬а ┬а ┬а ┬а "hi-IN": "рдорд╛рдл рдХрд░реЗрдВ, рдПрдЖрдИ рдореЙрдбрд▓ рд╕реЗ рдХрдиреЗрдХреНрдЯ рдирд╣реАрдВ рд╣реЛ рд╕рдХрд╛ред",
┬а ┬а ┬а ┬а ┬а ┬а "mr-IN": "рдХреНрд╖рдорд╕реНрд╡, рдПрдЖрдп рдореЙрдбреЗрд▓рд╢реА рдХрдиреЗрдХреНрдЯ рд╣реЛрдК рд╢рдХрд▓реЗ рдирд╛рд╣реА.",
┬а ┬а ┬а ┬а ┬а ┬а "kn-IN": "р▓Хр│Нр▓╖р▓ор▓┐р▓╕р▓┐, AI р▓ор▓╛р▓жр▓░р▓┐р▓Чр│Ж р▓╕р▓Вр▓кр▓░р│Нр▓Хр▓┐р▓╕р▓▓р│Б р▓╕р▓╛р▓зр│Нр▓пр▓╡р▓╛р▓Чр▓▓р▓┐р▓▓р│Нр▓▓.",
┬а ┬а ┬а ┬а ┬а ┬а "en-IN": "Sorry, I couldn't connect to the AI model. Check your API key and network.",
┬а ┬а ┬а ┬а }[language];
┬а ┬а ┬а ┬а 
┬а ┬а ┬а ┬а setError(err.message || fallback);
┬а ┬а ┬а ┬а setResponse(fallback);
┬а ┬а ┬а ┬а speak(fallback, language);
┬а ┬а }
┬а };

┬а return (
┬а ┬а <div className="max-w-xl mx-auto p-6 bg-white rounded-2xl shadow-lg border mt-6">
┬а ┬а ┬а <h2 className="text-2xl font-bold text-gray-800 flex items-center gap-2 mb-4">
┬а ┬а ┬а ┬а <Bot className="w-7 h-7 text-indigo-600" /> Smart Voice Assistant
┬а ┬а ┬а </h2>

┬а ┬а ┬а {/* LANGUAGE SELECTOR */}
┬а ┬а ┬а <div className="flex items-center gap-2 mb-4">
┬а ┬а ┬а ┬а <Globe className="w-5 h-5" />
┬а ┬а ┬а ┬а <select
┬а ┬а ┬а ┬а ┬а className="border p-2 rounded"
┬а ┬а ┬а ┬а ┬а value={language}
┬а ┬а ┬а ┬а ┬а onChange={(e) => {
┬а ┬а ┬а ┬а ┬а ┬а setLanguage(e.target.value);
┬а ┬а ┬а ┬а ┬а ┬а // Stop listening immediately when language changes
┬а ┬а ┬а ┬а ┬а ┬а if(isListening && recognitionRef.current) {
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а recognitionRef.current.abort();
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а setIsListening(false);
┬а ┬а ┬а ┬а ┬а ┬а }
┬а ┬а ┬а ┬а ┬а }}
┬а ┬а ┬а ┬а >
┬а ┬а ┬а ┬а ┬а <option value="en-IN">English</option>
┬а ┬а ┬а ┬а ┬а <option value="hi-IN">Hindi</option>
┬а ┬а ┬а ┬а ┬а <option value="mr-IN">Marathi</option>
┬а ┬а ┬а ┬а ┬а <option value="kn-IN">Kannada</option>
┬а ┬а ┬а ┬а </select>
┬а ┬а ┬а </div>

┬а ┬а ┬а <div className="flex justify-center mb-6">
┬а ┬а ┬а ┬а <button
┬а ┬а ┬а ┬а ┬а onClick={() => {
┬а ┬а ┬а ┬а ┬а ┬а // Cancel any ongoing speech before listening again
┬а ┬а ┬а ┬а ┬а ┬а speechSynthesis.cancel();
┬а ┬а ┬а ┬а ┬а ┬а setTranscript("");
┬а ┬а ┬а ┬а ┬а ┬а setResponse("");
┬а ┬а ┬а ┬а ┬а ┬а setError(null);
┬а ┬а ┬а ┬а ┬а ┬а setIsListening(!isListening);
┬а ┬а ┬а ┬а ┬а }}
┬а ┬а ┬а ┬а ┬а className={`p-5 rounded-full text-white transition shadow-xl ${
┬а ┬а ┬а ┬а ┬а ┬а isListening ? "bg-red-500 animate-pulse" : "bg-green-600"
┬а ┬а ┬а ┬а ┬а }`}
┬а ┬а ┬а ┬а ┬а disabled={loading}
┬а ┬а ┬а ┬а >
┬а ┬а ┬а ┬а ┬а {loading ? (
┬а ┬а ┬а ┬а ┬а ┬а <Loader2 className="w-10 h-10 animate-spin" />
┬а ┬а ┬а ┬а ┬а ) : isListening ? (
┬а ┬а ┬а ┬а ┬а ┬а <MicOff className="w-10 h-10" />
┬а ┬а ┬а ┬а ┬а ) : (
┬а ┬а ┬а ┬а ┬а ┬а <Mic className="w-10 h-10" />
┬а ┬а ┬а ┬а ┬а )}
┬а ┬а ┬а ┬а </button>
┬а ┬а ┬а </div>

┬а ┬а ┬а {/* USER TRANSCRIPT */}
┬а ┬а ┬а {(transcript || (isListening && !transcript)) && (
┬а ┬а ┬а ┬а <div className="bg-gray-50 p-4 rounded mb-4 border">
┬а ┬а ┬а ┬а ┬а <p className="font-semibold">You said:</p>
┬а ┬а ┬а ┬а ┬а <p className="text-gray-700">
┬а ┬а ┬а ┬а ┬а ┬а {transcript || (isListening ? "*Listening...*" : "")}
┬а ┬а ┬а ┬а ┬а </p>
┬а ┬а ┬а ┬а </div>
┬а ┬а ┬а )}

┬а ┬а ┬а {/* ERROR MESSAGE */}
┬а ┬а ┬а {error && (
┬а ┬а ┬а ┬а <div className="bg-red-50 p-4 border border-red-200 rounded mb-4 flex items-center gap-2">
┬а ┬а ┬а ┬а ┬а ┬а <AlertTriangle className="w-5 h-5 text-red-600" />
┬а ┬а ┬а ┬а ┬а ┬а <p className="text-red-700 font-medium">{error}</p>
┬а ┬а ┬а ┬а </div>
┬а ┬а ┬а )}

┬а ┬а ┬а {/* ASSISTANT RESPONSE */}
┬а ┬а ┬а {response && (
┬а ┬а ┬а ┬а <div className="bg-green-50 p-4 border border-green-200 rounded">
┬а ┬а ┬а ┬а ┬а <p className="font-semibold">Assistant:</p>
┬а ┬а ┬а ┬а ┬а {loading && response.length > 0 ? (
┬а ┬а ┬а ┬а ┬а ┬а <div className="flex items-center gap-2 text-green-700">
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а <Loader2 className="w-5 h-5 animate-spin" />
┬а ┬а ┬а ┬а ┬а ┬а ┬а ┬а <p>{response}</p>
┬а ┬а ┬а ┬а ┬а ┬а </div>
┬а ┬а ┬а ┬а ┬а ) : (
┬а ┬а ┬а ┬а ┬а ┬а <p className="text-green-700">{response}</p>
┬а ┬а ┬а ┬а ┬а )}
┬а ┬а ┬а ┬а </div>
┬а ┬а ┬а )}
┬а ┬а </div>
┬а );
};